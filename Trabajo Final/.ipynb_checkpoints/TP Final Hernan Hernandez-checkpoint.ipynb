{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce37359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb48388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine #Contiene el dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb709f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Guardo el dataset en el objeto wine\n",
    "wine = load_wine(as_frame=True)\n",
    "\n",
    "#Convierto el frame de sklearn en un dataframe pandas\n",
    "dataset = pd.DataFrame(data= np.c_[wine['data'], wine['target']],\n",
    "                     columns= wine['feature_names'] + ['target'])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02c9cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Un poco de analisis exploratorio\n",
    "columnas = dataset.columns\n",
    "print(columnas)\n",
    "\n",
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90fb62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar= dataset.groupby(['target'])['target'].count().reset_index(name=\"n\")\n",
    "bar= pd.DataFrame(bar)\n",
    "\n",
    "fig = plt.figure(figsize =(4,4))\n",
    "plt.bar(bar['target'],bar['n'])\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.title(\"Distribución de las obs. por target\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08a68b",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f9ca9",
   "metadata": {},
   "source": [
    " - el archivo de datos esta compuesto por variables numéricas lo cuál no implica aplicar una transformación one hot encode a ninguna de las variables.\n",
    " - Los predictores son 14 y una variable target.\n",
    " - La variable target toma 3 valores, 0, 1 y 2 y la mayoría de las observaciones corresponden a la categoría 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fe68b",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1eebe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Realizamos escalado y normalizacion de todos los predictores\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Selecciono solo los predictores\n",
    "\n",
    "x = dataset.drop('target', axis=1)\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "#Escalado\n",
    "\n",
    "escala=MinMaxScaler()\n",
    "\n",
    "escala.fit(x)\n",
    "\n",
    "x_escalada = escala.transform(x)\n",
    "\n",
    "#print(x_escalada)\n",
    "\n",
    "#Normalizado\n",
    "\n",
    "normalize = Normalizer()\n",
    "\n",
    "normalize.fit(x)\n",
    "\n",
    "x_normalizada = normalize.transform(x)\n",
    "\n",
    "#Estandarizado\n",
    "\n",
    "estandarizado = StandardScaler()\n",
    "\n",
    "estandarizado.fit(x)\n",
    "\n",
    "x_estandarizada = estandarizado.transform(x)\n",
    "\n",
    "\n",
    "#Corroboramos el estandarizado\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Comprobaciones de la estandarizacion\")\n",
    "print(\"------------------------------------\")\n",
    "print(x_estandarizada.mean(axis=0)) #No es 0, pero es muy cercano a 0\n",
    "print(x_estandarizada.std(axis=0)) #El desvio es 1 para todos los predictores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9493f",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11818c",
   "metadata": {},
   "source": [
    " - Con la función StandarScale() hacemos que los predictores se estandaricen, lo que implica que todos pasan a tener media 0 y desviación estándar 1. En el ejercicio la media no es 0, pero es muy cercana y por eso se acepta que esta correcto el estandarizado.\n",
    " - Con esta estrategia evitamos que por la unidad de medida del predictor, alguno tenga más peso que el resto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fffdf",
   "metadata": {},
   "source": [
    "### Division del dataset en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con los datos sin procesar\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_test,y_train, y_test = train_test_split(\n",
    "dataset.drop('target', axis=1),dataset['target'], random_state=123, test_size= 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727afd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuamos con los datos preprocesados\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Guardamos en el objeto y la variable target\n",
    "\n",
    "y = dataset['target'].astype(int)\n",
    "\n",
    "X_train_s,X_test_s,y_train_s, y_test_s = train_test_split(\n",
    "x_estandarizada,y, random_state=123, test_size= 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf20e06",
   "metadata": {},
   "source": [
    "## Entrenamos un modelo Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454479a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Instanciamos el modelo\n",
    "\n",
    "knn = KNeighborsClassifier(3)\n",
    "\n",
    "#Entrenamos el modelo\n",
    "\n",
    "original = knn.fit(X_train,y_train)\n",
    "preprocesado = knn.fit(X_train_s,y_train_s)\n",
    "\n",
    "#Hacemos predicciones\n",
    "\n",
    "y_predicha_original = original.predict(X_test)\n",
    "\n",
    "y_predicha_preprocesado = preprocesado.predict(X_test_s)\n",
    "\n",
    "#Obtenemos algunas métricas de error\n",
    "\n",
    "print(\"Accurracy\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"El accuracy obtenido sin escalar es:\", round(knn.score(X_test, y_test),3))\n",
    "print(\"El accuracy obtenido preprocesado es:\", round(knn.score(X_test_s, y_test_s),3))\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "#Obtenemos el roc_auc\n",
    "\n",
    "print(\"ROC_AUC\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"El roc_auc obtenido sin escalar es:\", round(roc_auc_score(y_test,original.predict_proba(X_test),multi_class= 'ovo'),3))\n",
    "print(\"El roc_auc obtenido preprocesado es:\", round(roc_auc_score(y_test,preprocesado.predict_proba(X_test_s),multi_class= 'ovo'),3))\n",
    "print(\"--------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e81ac3",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324efda9",
   "metadata": {},
   "source": [
    " - Las dos métricas elegidas para evaluar el __modelo KNN__, muestran una __mejor performance__ cuando el modelo utiliza los __datos estandarizados__, llegando a un accuracy de __0.97__ y el roc_auc de __0.99__, vs los 0.22 y 0.5 obtenido con los datos sin estandarizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c5ec9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Obtenemos la matriz de confusión para ambos dataset (original y preprocesado)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Matriz de confusion datos originales\n",
    "cm = confusion_matrix(y_test,y_predicha_original)\n",
    "\n",
    "#Matriz de confusión datos preprocesados\n",
    "\n",
    "cm_s = confusion_matrix(y_test,y_predicha_preprocesado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90046b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15,10))\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].set_title(\"Preprocesado\")\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=knn.classes_)\n",
    "\n",
    "disp.plot(cmap=plt.cm.GnBu,ax=ax[0])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_s,\n",
    "                              display_labels=knn.classes_)\n",
    "\n",
    "disp.plot(cmap=plt.cm.GnBu,ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec000ebd",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65973471",
   "metadata": {},
   "source": [
    " - La __matriz de confusión__ muestra de forma más explicita lo que describiamos en el apartado anterior. Cuando el __modelo KNN__ utiliza los datos sin estandarizar (imágen de la izquierda), clasifica las 36 observaciones del dataset de test en 0, cuando __solo 8__ debieron ser clasificadas con este valor. \n",
    " - En cambio, cuando __modelo KNN__ utiliza los datos estandarizados (imágen de la derecha), __clasifica correctamente__ __35 de las 36__ observaciones del __dataset de test__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc65729",
   "metadata": {},
   "source": [
    "## Entrenamos un modelo de DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de40e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importamos el modelo\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#generamos una semilla aleatoria\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "#Instanciamos el modelo\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "#Entrenamos el modelo\n",
    "\n",
    "arbol_original = tree.fit(X_train,y_train)\n",
    "arbol_preprocesado = tree.fit(X_train_s,y_train_s)\n",
    "\n",
    "#Hacemos predicciones\n",
    "\n",
    "y_arbol_original = arbol_original.predict(X_test)\n",
    "\n",
    "y_arbol_preprocesado = arbol_preprocesado.predict(X_test_s)\n",
    "\n",
    "#Obtenemos algunas métricas de error\n",
    "\n",
    "print(\"Accurracy\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"El accuracy obtenido sin escalar es:\", round(tree.score(X_test, y_test),3))\n",
    "print(\"El accuracy obtenido preprocesado es:\", round(tree.score(X_test_s, y_test_s),3))\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "#Obtenemos el roc_auc\n",
    "\n",
    "print(\"ROC_AUC\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"El roc_auc obtenido sin escalar es:\", round(roc_auc_score(y_test,arbol_original.predict_proba(X_test),multi_class= 'ovo'),3))\n",
    "print(\"El roc_auc obtenido preprocesado es:\", round(roc_auc_score(y_test_s,arbol_preprocesado.predict_proba(X_test_s),multi_class= 'ovo'),3))\n",
    "print(\"--------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8a07a",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243680a",
   "metadata": {},
   "source": [
    " - Al igual que lo observado en el entrenamiento del modelo KNN, aqui podemos observar que tanto el accuracy como el roc_auc muestran mejor performance con los datos preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485550c5",
   "metadata": {},
   "source": [
    "### Optimizamos el hiperparámetro max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ef91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimizamos el parámetro max_depth, utlizando GridSeacrhCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV #grid search\n",
    "\n",
    "#generamos una semilla aleatoria\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Hiperoptimizacion del max_depth\n",
    "\n",
    "param_grid = { \n",
    "    'max_depth' : [3,4,5,6,7],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "#Modelo\n",
    "\n",
    "arbol = DecisionTreeClassifier()\n",
    "\n",
    "model_grid = GridSearchCV(estimator=arbol, param_grid=param_grid, cv= 5, refit= True)\n",
    "\n",
    "#Entrenamos el modelo con datos preprocesados\n",
    "\n",
    "model_grid_fit = model_grid.fit(X_train_s, y_train_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fa3d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Buscamos el mejor estimador\n",
    "\n",
    "print(\"Mejor estimador modelo con datos preprocesados\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(model_grid_fit.best_estimator_)\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db466aa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Vemos los errores\n",
    "\n",
    "resultados = pd.DataFrame(model_grid_fit.cv_results_)\n",
    "resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c55957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ahora vamos a entrenar nuestro mejor modelo con los parámetros optimizados\n",
    "\n",
    "#importamos el modelo\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#generamos una semilla aleatoria\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "#Instanciamos el modelo\n",
    "tree_opt = DecisionTreeClassifier(max_depth=6,criterion='entropy')\n",
    "\n",
    "#Entrenamos el modelo\n",
    "\n",
    "arbol_optimizado = tree_opt.fit(X_train_s,y_train_s)\n",
    "\n",
    "#Hacemos predicciones\n",
    "\n",
    "y_arbol_opt = arbol_optimizado.predict(X_test_s)\n",
    "\n",
    "#Analizamos las metricas de error\n",
    "\n",
    "print(\"Accurracy\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"El accuracy obtenido con el modelo optimizado es:\", round(tree_opt.score(X_test_s, y_test_s),3))\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "print(\"ROC_AUC\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"El roc_auc obtenido con el modelo optimizado es:\", round(roc_auc_score(y_test_s,arbol_optimizado.predict_proba(X_test_s),multi_class= 'ovo'),3))\n",
    "print(\"--------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d08476",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfef10",
   "metadata": {},
   "source": [
    " - Se __optimizaron 2 hiperparámetros__ del modelo de DecisionTree, el __max_depth__ y el __criterio de división__ mediante la estrategia __GridSearchCV__.\n",
    " - La tabla _resultados_ muestra los modelos entrenados con distintas combicaciones de los hiperparámetros. Asmismo muestra el promedio del accuracy ordenados de mayor a menor.\n",
    " - De allí podemos saber que el modelo más optimo es el que setea los parámtros __max_depth__ en __6__ y __criterion__ __entropy__ .\n",
    " - En este caso particuar la __optimización de los parámetros__ no implicó una mejora en las métricas de error, incluso se obtuvo un menor accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902cd7e",
   "metadata": {},
   "source": [
    "### Vemos la importancia de los predictores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd9ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vemos la importancia de los predictores\n",
    "\n",
    "importancia_predictores = pd.DataFrame(\n",
    "                            {'predictor': x.columns,\n",
    "                             'importancia':tree.feature_importances_}\n",
    "                            )\n",
    "importancia_predictores.sort_values('importancia', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77eb7f7",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee8d6a",
   "metadata": {},
   "source": [
    " - Aquí podemos observar que los __feature__ _proline_ y _od280/od315_of_diluted_wines_ son los que mayor importancia tuvieron en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca6f7a",
   "metadata": {},
   "source": [
    "## Aplicamos un modelo de reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b89460",
   "metadata": {},
   "source": [
    "### Utilizamos el Análisis de la componente principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cb1d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import mglearn\n",
    "\n",
    "#Instanciamos el modelo PC\n",
    "pca=PCA(n_components=2)\n",
    "\n",
    "#Entrenamoe el modelo\n",
    "pca.fit(x_estandarizada)\n",
    "\n",
    "#Transformamos los datos estandarizados\n",
    "transformados = pca.transform(x_estandarizada)\n",
    "print(x.shape)\n",
    "print(transformados.shape)\n",
    "\n",
    "#hacemos un gráfico\n",
    "\n",
    "\n",
    "mglearn.discrete_scatter(transformados[:,0], transformados[:,1], dataset['target'])\n",
    "plt.legend(dataset['target'].unique(), loc='lower right')\n",
    "plt.xlabel('Componente pca 1')\n",
    "plt.ylabel('Componente pca 2')\n",
    "\n",
    "#Creamos un dataframe con los predictores que quedan incluidos en ambos componentes principales\n",
    "\n",
    "predictores = pd.DataFrame(pca.components_.T, columns= ['PC-1','PC-2'], index= x.columns)\n",
    "\n",
    "predictores.sort_values('PC-1',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------')\n",
    "print('Porcentaje de varianza explicada por cada componente')\n",
    "print('----------------------------------------------------')\n",
    "print(pca.explained_variance_ratio_)\n",
    "print('--------------------------------------------------------------')\n",
    "print('Porcentaje de varianza acumulada explicada por cada componente')\n",
    "print('--------------------------------------------------------------')\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b158cf1",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b795494",
   "metadata": {},
   "source": [
    " - En el cuadro anterior podemos observar cuáles __feature__ dentro de los componentes explican la mayor cantidad de varianza.\n",
    " - En nuestro análisis podemos ver que las variables _flavonoid_, _total_phenols_ y _od280/od315_of_diluted_wines_ tienen el mayor peso en la PC-1. Mientras que en PC-2 tienen más peso _color_intensity_, _alcohol_ y _proline_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d84b3",
   "metadata": {},
   "source": [
    "## Aplicamos el modelo KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe71f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importamos la libreria\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "#plantamos una semilla aleatoria\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "#Instanciamos el modelo\n",
    "\n",
    "model_k_means = KMeans(n_clusters=3, max_iter=2000) \n",
    "\n",
    "#Instanciamos el momento 0\n",
    "t0 = time.time()\n",
    "\n",
    "#Entrenamos el modelo\n",
    "model_k_means.fit(transformados)\n",
    "\n",
    "#Capturamos el tiempo luego de entrenar el modelo menos el momento 0\n",
    "t_batch = time.time() - t0\n",
    "\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"---Tiempo de ejecución---\")\n",
    "print(\"-------\",round(t_batch,2),\"--------\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "#Generamos las predicciones\n",
    "\n",
    "pred_k_means = model_k_means.predict(transformados)\n",
    "\n",
    "#Obtenemos las métricas de error\n",
    "\n",
    "score=metrics.adjusted_rand_score(dataset['target'], pred_k_means)\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"------Indice Rand------------\")\n",
    "print(\"El score obtenido es: \",round(score,3))\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "#Graficamos las predicciones\n",
    "\n",
    "\n",
    "plt.scatter(transformados[:, 0], transformados[:, 1], c=pred_k_means)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2c6cc",
   "metadata": {},
   "source": [
    "## Aplicamos el modelo KMeans mini-batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6a582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importamos el modelo de la libreria cluster\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "#plantamos una semilla aleatoria\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "#Instanciamos el momento 0\n",
    "t1 = time.time()\n",
    "\n",
    "#Instanciamos el mismo modelo pero Mini batches\n",
    "\n",
    "k_means_mb = MiniBatchKMeans(n_clusters=3, max_iter=2000)\n",
    "\n",
    "#Capturamos el tiempo luego de entrenar el modelo menos el momento 0\n",
    "t_batch_1 = time.time() - t1\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"---Tiempo de ejecución---\")\n",
    "print(\"---------\",round(t_batch_1,2),\"----------\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "#Entrenamos el modelo tulizando los datos escalados\n",
    "\n",
    "k_means_mb.fit(transformados)\n",
    "\n",
    "#Creamos las predicciones del modelo\n",
    "\n",
    "pred_k_means_mb = k_means_mb.predict(transformados)\n",
    "\n",
    "#Obtenemos las métricas de error\n",
    "\n",
    "score_mb= metrics.adjusted_rand_score(dataset['target'], pred_k_means_mb)\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"------Indice Rand------------\")\n",
    "print(\"El score obtenido es: \",round(score_mb,3))\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "#Graficamos las predicciones\n",
    "\n",
    "\n",
    "plt.scatter(transformados[:, 0], transformados[:, 1], c=pred_k_means_mb)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c74829",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309211d",
   "metadata": {},
   "source": [
    " - Podemos observar que el modelo K-Means Mini Batch nos permite agrupar los datos en cluster con una menor precisión que K-means (score 0.847 vs 0.895). Ahora bien, desde el punto de vista del tiempo de ejecución obtenido con la __libreria time__, podemos observar que el modelo K-means tardo 0.4 s y el modelo K-means Mini Batch tarda 0.0 s. \n",
    " - En conclusión, el modelo __K-means Mini Batch__ es más __eficiente__ obteniendo métricas de error solo __levemente mayores__ al __modelo K-means__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7dabf6",
   "metadata": {},
   "source": [
    "### Bonus Track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df417796",
   "metadata": {},
   "source": [
    "Quizás un primer paso que debería haber implementado es utilizar una matriz de correlación para observar la existencia de __colinealidad__ y consecuencia excluir los predictores que este correlacionados.\n",
    "En esta etapa analizaremos los predictores mediante una __matriz de correlación__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf8d83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Armamos un data frame con los predictores\n",
    "data_corr = pd.DataFrame(x_estandarizada, columns = x.columns)\n",
    "\n",
    "#Mostramos la matriz de correlacion\n",
    "\n",
    "corr_mat = data_corr.corr()\n",
    "\n",
    "#Graficamos la matriz de correlacion\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "\n",
    "sn.heatmap(corr_mat, annot=True,cmap=\"crest\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397a14d",
   "metadata": {},
   "source": [
    "#### Observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95f540",
   "metadata": {},
   "source": [
    " - En la matriz se puede observar una fuerte correlación entre las siguientes variables independientes/predictores:\n",
    "     - proline y alcohol: 0.64\n",
    "     - od280/od315_of_diluted_wines y total_phenols\n",
    "     - proanthocyanins y total_phenols\n",
    "     - flavanoids y total_phenols\n",
    "     - od280/od315_of_diluted_wines y flavanoids\n",
    "     \n",
    " - Teniendo en cuenta lo anterior podría excluirse del modelo __total_phenols__, __flavanoids__, __od280/od315_of_diluted_wines__. También, como se hizo en este TP, aplicar un __PCA__ ayuda a reducir la dimensionalidad y por lo tanto controlar lo __colinealidad/multicolinealidad__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
